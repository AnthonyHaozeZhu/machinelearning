{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习实验六 --决策树\n",
    " \n",
    "\n",
    "## 实验要求\n",
    "- 基本要求\n",
    "    1. 基于 Watermelon-train1数据集（只有离散属性），构造ID3决策树；\n",
    "    2. 基于构造的 ID3 决策树，对数据集 Watermelon-test1进行预测，输出分类精度；\n",
    "- 中级要求\n",
    "    1. 对数据集Watermelon-train2，构造C4.5或者CART决策树，要求可以处理连续型属性；\n",
    "    2. 对测试集Watermelon-test2进行预测，输出分类精度；\n",
    "\n",
    "\n",
    "## 导入数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "def readfile(name):\n",
    "    df = pd.read_csv(name, error_bad_lines = False, encoding = 'gbk')\n",
    "    temp = np.array(df).tolist()\n",
    "    for i in temp:\n",
    "        i.pop(0)\n",
    "    return temp\n",
    "\n",
    "train1 = readfile(\"Watermelon-train1.csv\")\n",
    "train2 = readfile(\"Watermelon-train2.csv\")\n",
    "test1 = readfile(\"Watermelon-test1.csv\")\n",
    "test2 = readfile(\"Watermelon-test2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算信息增益\n",
    "- 统计某结果数据发生的频率，每项的信息以字典的形似存储\n",
    "- 计算信息熵\n",
    "- 返回信息熵和分类信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information(data):\n",
    "    dic = {}\n",
    "    for i in data:\n",
    "        current = i[-1] #取出最后的结果\n",
    "        if current not in dic.keys(): \n",
    "            dic[current] = 1 #创建一个新的类别\n",
    "        else:\n",
    "            dic[current] += 1 #原有类别+1\n",
    "    result = 0.0\n",
    "    for key in dic:\n",
    "        prob = float(dic[key]) / len(data)\n",
    "        result -= prob * math.log(prob,2) \n",
    "    return result, dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将数据按照某一种类的属性重新分类，并将该行属性删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, index, kind):\n",
    "    ls = []\n",
    "    for temp in data:\n",
    "        if temp[index] == kind:\n",
    "            t = temp[0: index]\n",
    "            t = t + temp[index + 1: ]\n",
    "            ls.append(t)\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算信息熵和信息增益并返回最优的分类方式，对数据的每项指标做以下的过程\n",
    "    1. 抽取该项数据的所有信息\n",
    "    2. 按照该项数据的类别信息将数据集划分成多个子数据集\n",
    "    3. 计算每个数据集的信息熵\n",
    "    5. 计算该项数据的信息增益\n",
    "    6. 根据信息增益选择最好的分类项目，返回该项目的类别号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestFeatureToSplit(data):\n",
    "    base, mm= information(data) #原始的信息熵\n",
    "    best = 0\n",
    "    bestindex = -1\n",
    "    for i in range(len(data[0]) - 1):\n",
    "        #抽取该列数据的所有信息\n",
    "        ls = [index[i] for index in data]\n",
    "        feture = set(ls) \n",
    "        #计算该列的信息增益\n",
    "        temp = 0.0\n",
    "        for value in feture:\n",
    "            datatemp = split(data, i, value)\n",
    "            prob = len(datatemp) / float(len(data))\n",
    "            t, mm = information(datatemp)\n",
    "            temp += prob * t\n",
    "        infoGain = base - temp\n",
    "        #根据信息增益挑选 \n",
    "        if infoGain > best:\n",
    "            best = infoGain\n",
    "            bestindex = i\n",
    "    return bestindex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 递归构建$ID3$决策树\n",
    "- 决策树的生成\n",
    "\t1. 从根节点开始，计算所有可能特征的信息增益，选择信息增益最大的特征作为划分该节点的特征，根据该特征的不同取值建立子节点；\n",
    "\t2. 在对子节点递归地调用以上方法，直到达到停止条件，得到⼀个决策树。\n",
    "- 迭代停止条件\n",
    "    1. 当前结点所有样本都属于同⼀类别；\n",
    "    2. 当前结点的所有属性值都相同，没有剩余属性可用来进一步划分样本；\n",
    "    3. 达到最大树深；\n",
    "    4. 达到叶子结点的最小样本数；\n",
    "- 具体实现\n",
    "    1. 首先取出该数据集的类别信息\n",
    "    2. 统计处类别信息以及数量，如果只有一个类别，返回该类别\n",
    "    3. 计算最优划分的索引\n",
    "    4. 初始化子树\n",
    "    5. 当前已经选择的特征不再参与分类，将该特征删除\n",
    "    6. 计算剩余特征的集合\n",
    "    7. 对于每个分支，进行递归\n",
    "    8. 返回值为子树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify1(data, labels):\n",
    "    typelist = [index[-1] for index in data] #取出该数据集的分类\n",
    "    nothing, typecount = information(data) #计算出类别种类以及数量\n",
    "    if len(typecount) == 1: #如果只有一个类别\n",
    "        return typelist[0]\n",
    "    bestindex = chooseBestFeatureToSplit(data)  # 最优划分属性的索引\n",
    "    bestlabel = labels[bestindex]\n",
    "    Tree = {bestlabel: {}}\n",
    "    temp = labels[:]\n",
    "    del (temp[bestindex])  # 已经选择的特征不再参与分类，将该类别删除\n",
    "    feature = [example[bestindex] for example in data]\n",
    "    unique = set(feature)  # 该属性所有可能取值，也就是节点的分支\n",
    "    for i in unique:  \n",
    "        temp = temp[:] \n",
    "        Tree[bestlabel][i] = classify1(split(data, bestindex, i), temp)\n",
    "    return Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对测试集利用$ID3$决策树进行分类\n",
    "- 取出当前树的根节点\n",
    "- 利用跟节点信息查询当前输入数据输入内容\n",
    "- 如果查询出来的分支是叶节点，返回该值\n",
    "- 如果不是叶节点，递归查询子树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成决策树如下: {'纹理': {'模糊': '否', '清晰': {'根蒂': {'稍蜷': '是', '蜷缩': '是', '硬挺': '否'}}, '稍糊': {'色泽': {'青绿': '否', '乌黑': {'敲声': {'沉闷': '否', '浊响': '是'}}, '浅白': '否'}}}}\n"
     ]
    }
   ],
   "source": [
    "def run1(testdata, tree, labels):\n",
    "    firstStr = list(tree.keys())[0]\n",
    "    secondDict = tree[firstStr]\n",
    "    featIndex = labels.index(firstStr)\n",
    "    result = ''\n",
    "    for key in list(secondDict.keys()): \n",
    "         if testdata[featIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == 'dict':  # 该分支不是叶子节点，递归\n",
    "                result = run1(testdata, secondDict[key], labels)\n",
    "            else:\n",
    "                result = secondDict[key]\n",
    "    return result\n",
    "\n",
    "def getresult(train, test, labels):\n",
    "    ls = []\n",
    "    tree = classify1(train, labels)\n",
    "    print(\"生成决策树如下:\", tree)\n",
    "    for index in test:\n",
    "        ls.append(run1(index, tree, labels))\n",
    "    return ls\n",
    "\n",
    "labels1 = ['色泽', '根蒂', '敲声', '纹理', '好瓜']\n",
    "result1 = getresult(train1, test1, labels1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3分类器对于test1数据集的准确率是： 70.00%\n"
     ]
    }
   ],
   "source": [
    "def simrate(data, predict):\n",
    "    num = 0\n",
    "    for i in range(len(data)):\n",
    "        if data[i][-1] == predict[i]:\n",
    "            num +=1\n",
    "    return format(num / len(data), '.2%')\n",
    "print(\"ID3分类器对于test1数据集的准确率是：\", simrate(test1, result1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，用这种方法得到树分类树对于小规模数据的分类效果相对较好。\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用$C4.5$算法构建决策树\n",
    "信息增益作为划分准则存在的问题：<br/>\n",
    "信息增益偏向于选择取值较多的特征进行划分。⽐如学号这个特征，每个学生都有一个不同的学号，如果根据学号对样本进行分类，则每个学生都属于不同的类别，这样是没有意义的。而C4.5在生成过程中，用信息增益比来选择特征，可以校正这个问题。<br/>\n",
    "信息增益比 = 惩罚参数 * 信息增益，即 $g_R(D,A) = \\frac{g(D,A)}{H_A(D)}$，其中的$H_A(D)$，对于样本集合D，将当前特征A作为随机变量（取值是特征A的各个特征值），求得的经验熵。<br/>\n",
    "信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。<br/>\n",
    "惩罚参数：数据集D以特征A作为随机变量的熵的倒数，即：将特征A取值相同的样本划分到同一个子集中，$惩罚参数 = \\frac{1}{H_A(D)}=\\frac{1}{-\\sum_{(i = 1)}^{n}\\frac{|D_i|}{|D|}\\log_2\\frac{|D_i|}{|D|}}$ <br/>\n",
    "- 将连续属性离散化<br/>\n",
    "采用二分法，把数据由小到大排列，选择每个区间的中位点位取值，左侧区间为不大于该点的值，右侧区间为大于该点的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Division(data):\n",
    "    ls = data[:]\n",
    "    ls.sort()\n",
    "    result = []\n",
    "    for i in range(len(ls) - 1):\n",
    "        result.append((data[i + 1] + data[i]) / 2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将数据按照某一种类的属性或者数值的区间重新分类；method = 0 按照区间左侧分类，method = 1 按照区间右侧分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split2(data, index, kind, method):\n",
    "    ls = []\n",
    "    if method == 0:\n",
    "        for temp in data:\n",
    "            if temp[index] <= kind:\n",
    "                t = temp[0 : index]\n",
    "                t = t + temp[index + 1 : ]\n",
    "                ls.append(t)\n",
    "    else:\n",
    "        for temp in data:\n",
    "            if temp[index] > kind:\n",
    "                t = temp[0 : index]\n",
    "                t = t + temp[index + 1 : ]\n",
    "                ls.append(t)\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算信息熵和信息增益并返回最优的分类方式，对数据的每项指标做以下的过程\n",
    "    1. 抽取该项数据的所有信息\n",
    "    2. 计算每一类的信息熵\n",
    "    3. 按照该项数据的类别信息将数据集划分成多个子数据集\n",
    "    4. 计算每个数据集的信息熵\n",
    "    5. 计算该项数据的信息增益比\n",
    "    6. 根据信息增益选择最好的分类项目，返回该项目的类别号\n",
    "    7. 对连续型数据，计算分割值，求出信息增益比最小的点作为分割点返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestFeatureToSplit2(data):\n",
    "    base, mm= information(data) #原始的信息熵\n",
    "    info = []\n",
    "    for j in range(len(data[0]) - 1):\n",
    "        dic = {}\n",
    "        for i in data:\n",
    "            current = i[j] #取出最后的结果\n",
    "            if current not in dic.keys(): \n",
    "                dic[current] = 1 #创建一个新的类别\n",
    "            else:\n",
    "                dic[current] += 1 #原有类别+1\n",
    "        result = 0.0\n",
    "        for key in dic:\n",
    "            prob = float(dic[key]) / len(data)\n",
    "            result -= prob * math.log(prob,2) \n",
    "        info.append(result)\n",
    "    best = 0\n",
    "    bestindex = -1\n",
    "    bestpartvalue = None #如果是离散值，使用该值进行分割\n",
    "    for i in range(len(data[0]) - 1):\n",
    "        #抽取该列数据的所有信息\n",
    "        ls = [index[i] for index in data]\n",
    "        feture = set(ls) \n",
    "        #计算该列的信息增益\n",
    "        temp = 0.0\n",
    "        if type(ls[0]) == type(\"a\"):#判断是否时离散的\n",
    "            for value in feture:\n",
    "                datatemp = split(data, i, value)\n",
    "                prob = len(datatemp) / float(len(data))\n",
    "                t, mm = information(datatemp)\n",
    "                temp += prob * t\n",
    "        else:\n",
    "            ls.sort()\n",
    "            min = float(\"inf\")\n",
    "            for j in range(len(ls) - 1):\n",
    "                part = (ls[j + 1] + ls[j]) / 2 #计算划分点\n",
    "                left = split2(data, i, part, 0)\n",
    "                right = split2(data, i, part, 1)\n",
    "                temp1, useless = information(left)\n",
    "                temp2, useless = information(right)\n",
    "                temp = len(left) / len(data) * temp1 + len(right) / len(data) * temp2\n",
    "                if temp < min:\n",
    "                    min = temp\n",
    "                    bestpartvalue = part\n",
    "            temp = min\n",
    "        infoGain = base - temp\n",
    "        #根据信息增益挑选 \n",
    "        if info[i] != 0:\n",
    "            if infoGain / info[i] >= best:\n",
    "                best = infoGain / info[i]\n",
    "                bestindex = i\n",
    "    return bestindex, bestpartvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 递归构建$C4.5$决策树\n",
    "- 决策树的生成\n",
    "\t1. 从根节点开始，计算所有可能特征的信息增益，选择信息增益最大的特征作为划分该节点的特征，根据该特征的不同取值建立子节点；\n",
    "\t2. 在对子节点递归地调用以上方法，直到达到停止条件，得到⼀个决策树。\n",
    "- 迭代停止条件\n",
    "    1. 当前结点所有样本都属于同⼀类别；\n",
    "    2. 当前结点的所有属性值都相同，没有剩余属性可用来进一步划分样本；\n",
    "    3. 达到最大树深；\n",
    "    4. 达到叶子结点的最小样本数；\n",
    "- 具体实现\n",
    "    1. 首先取出该数据集的类别信息\n",
    "    2. 统计处类别信息以及数量，如果只有一个类别，返回该类别\n",
    "    3. 计算最优划分的索引\n",
    "    4. 初始化子树\n",
    "    5. 当前已经选择的特征如果是离散的便不再参与分类，将该特征删除；如果是连续的不删除该特征，以分界点构建左子树和右子树\n",
    "    6. 计算剩余特征的集合\n",
    "    7. 对于每个分支，进行递归\n",
    "    8. 返回值为子树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify2(data, labels):\n",
    "    typelist = [index[-1] for index in data] #取出该数据集的分类\n",
    "    nothing, typecount = information(data) #计算出类别种类以及数量\n",
    "    if typecount == len(typelist): #如果只有一个类别\n",
    "        return typelist[0]\n",
    "    bestindex, part = chooseBestFeatureToSplit2(data)  # 最优划分属性的索引\n",
    "    if bestindex == -1:\n",
    "        return \"是\"\n",
    "    if type([t[bestindex] for t in data][0]) == type(\"a\"):#判断是否时离散的\n",
    "        bestlabel = labels[bestindex]\n",
    "        Tree = {bestlabel: {}}\n",
    "        temp = copy.copy(labels)\n",
    "        feature = [example[bestindex] for example in data]\n",
    "        del (temp[bestindex])  # 已经选择的特征不再参与分类，将该类别删除\n",
    "        unique = set(feature)  # 该属性所有可能取值，也就是节点的分支\n",
    "        for i in unique:  \n",
    "            s = temp[:] \n",
    "            Tree[bestlabel][i] = classify2(split(data, bestindex, i), s)\n",
    "    else: #连续的变量\n",
    "        bestlabel = labels[bestindex] + \"<\" + str(part)\n",
    "        Tree = {bestlabel: {}}\n",
    "        temp = labels[:]\n",
    "        del(temp[bestindex])\n",
    "        leftdata = split2(data, bestindex, part, 0)\n",
    "        Tree[bestlabel][\"是\"] = classify2(leftdata, temp)\n",
    "        rightdata = split2(data, bestindex, part, 1)\n",
    "        Tree[bestlabel][\"否\"] = classify2(rightdata, temp)\n",
    "    return Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对测试集利用$C4.5$决策树进行分类\n",
    "- 取出当前树的根节点\n",
    "- 利用跟节点信息查询当前输入数据输入内容\n",
    "- 如果是离散值\n",
    "    1. 如果查询出来的分支是叶节点，返回该值\n",
    "    2. 如果不是叶节点，递归查询子树\n",
    "- 如果是非离散值\n",
    "    1. 取出节点值与现在数据进行比较\n",
    "    2. 如果大于以“否”为标签，否则以“是”为标签\n",
    "    3. 如果查询出来的分支是叶节点，返回该值\n",
    "    4. 如果不是叶节点，递归查询子树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成决策树如下: {'纹理': {'模糊': {'密度<0.294': {'是': '是', '否': '是'}}, '清晰': {'根蒂': {'稍蜷': {'密度<0.38149999999999995': {'是': '是', '否': {'色泽': {'青绿': '是', '乌黑': '是'}}}}, '蜷缩': {'密度<0.5820000000000001': {'是': '是', '否': {'敲声': {'沉闷': {'色泽': {'青绿': '是', '乌黑': '是'}}, '浊响': {'色泽': {'青绿': '是', '乌黑': '是'}}}}}}, '硬挺': '是'}}, '稍糊': {'敲声': {'沉闷': {'密度<0.6615': {'是': '是', '否': {'根蒂': {'稍蜷': '是', '蜷缩': '是'}}}}, '浊响': {'密度<0.56': {'是': '是', '否': '是'}}}}}}\n"
     ]
    }
   ],
   "source": [
    "def run2(data, tree, labels):\n",
    "    firstStr = list(tree.keys())[0]  # 根节点\n",
    "    firstLabel = firstStr\n",
    "    t = str(firstStr).find('<') #查看是否是连续型\n",
    "    if t > -1:  # 如果是连续型的特征\n",
    "        firstLabel = str(firstStr)[ : t]\n",
    "    secondDict = tree[firstStr]\n",
    "    featIndex = labels.index(firstLabel)  # 跟节点对应的属性\n",
    "    result = ''\n",
    "    for key in list(secondDict.keys()):  # 对每个分支循环\n",
    "        if type(data[featIndex]) == type(\"a\"):\n",
    "            if data[featIndex] == key:  # 测试样本进入某个分支\n",
    "                if type(secondDict[key]).__name__ == 'dict':  # 该分支不是叶子节点，递归\n",
    "                    result = run2(data, secondDict[key], labels)\n",
    "                else:  # 如果是叶子， 返回结果\n",
    "                    result = secondDict[key]\n",
    "        else:\n",
    "            value = float(str(firstStr)[t + 1 : ])\n",
    "            if data[featIndex] <= value:\n",
    "                if type(secondDict['是']).__name__ == 'dict':  # 该分支不是叶子节点，递归\n",
    "                    result = run2(data, secondDict['是'], labels)\n",
    "                else:  # 如果是叶子， 返回结果\n",
    "                    result = secondDict['是']\n",
    "            else:\n",
    "                if type(secondDict['否']).__name__ == 'dict':  # 该分支不是叶子节点，递归\n",
    "                    result = run2(data, secondDict['否'], labels)\n",
    "                else:  # 如果是叶子， 返回结果\n",
    "                    result = secondDict['否']\n",
    "    return result\n",
    "\n",
    "def getresult2(train, test, labels):\n",
    "    ls = []\n",
    "    tree = classify2(train, labels)\n",
    "    print(\"生成决策树如下:\", tree)\n",
    "    for index in test:\n",
    "        ls.append(run2(index, tree, labels))\n",
    "    return ls\n",
    "\n",
    "labels2 = [\"色泽\", \"根蒂\", \"敲声\", \"纹理\", \"密度\", \"好瓜\"]\n",
    "result2 = getresult2(train2, test2, labels2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算准确率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5分类器对于test2数据集的准确率是： 60.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"C4.5分类器对于test2数据集的准确率是：\", simrate(test2, result2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，在不剪枝的情况下C4.5预测的准确率稍较为一般"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
